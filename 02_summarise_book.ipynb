{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have placed some of the less interesting code in the `convert_book_to_text.py` file to try to keep the notebook flowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for the reload of all the functions in the file convert_pdf_to_text.py\n",
    "from importlib import reload\n",
    "import convert_book_to_text\n",
    "\n",
    "reload(convert_book_to_text)\n",
    "from convert_book_to_text import find_chapter_pages, \\\n",
    "                                 find_page_starting_with,\\\n",
    "                                 extract_text_from_pages_inclusive, \\\n",
    "                                 build_chapter_dictionary, \\\n",
    "                                 remove_chapter_header, \\\n",
    "                                 chunk_text, \\\n",
    "                                 find_bad_csv_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For copyright reasons I have not included the ebook in the repo. You should buy your own copy if you want to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n",
    "input_file = Path('./inputs/Truth to Power_ My Three Years Inside Eskom.pdf')\n",
    "input_file = Path(input_file)\n",
    "# Instantiate PdfReader object\n",
    "pdf = PyPDF2.PdfReader(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very specific to the format of the book I am precessing. Not making this general is a design decision. This repo needs back and forward with a human\n",
    "# to work so spending too much time generalising functions seems like a waste of time.\n",
    "\n",
    "# Note: This is quite slow. It takes about 30 seconds to run on my machine.\n",
    "pages_with_numbers = find_chapter_pages(pdf)\n",
    "pages_with_numbers.append(find_page_starting_with(pdf, \"Index\"))\n",
    "# Create a dictionary where each chapter is saved with its start and end page\n",
    "chapter_start_and_end = build_chapter_dictionary(pages_with_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is for illustration. It allows the user to see what is happening in the main loop below, but for a single chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop. Chapters start at 1 and end at 31. chapter_range_end should be set to the number after the last chapter you want to process so, if you are processing the last chapter, chapter 31, only you would set `chapter_range_start = 31` and `chapter_range_end = 32`.\n",
    "\n",
    "The output of this is a list of CSV files in the folder `./output/`. A separate file is created for each chunk. \n",
    "\n",
    "The function sets a chunk size of 800 words. There is no science in determining how long this should be but we need to ensure there are sufficient tokens for the input, the system command and the output. I started with a chunk size of 1000 words but I found there were times where GPT3.5 would truncate the output. 800 seemed reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chapter:  28 from page 204 to page 208\n",
      "processing chunk 0 out of 3\n",
      "processing chunk 1 out of 3\n",
      "processing chunk 2 out of 3\n",
      "processing chapter:  29 from page 209 to page 212\n",
      "processing chunk 0 out of 3\n",
      "processing chunk 1 out of 3\n",
      "processing chunk 2 out of 3\n",
      "processing chapter:  30 from page 213 to page 218\n",
      "processing chunk 0 out of 4\n",
      "processing chunk 1 out of 4\n",
      "processing chunk 2 out of 4\n",
      "processing chunk 3 out of 4\n",
      "processing chapter:  31 from page 219 to page 226\n",
      "processing chunk 0 out of 6\n",
      "processing chunk 1 out of 6\n",
      "processing chunk 2 out of 6\n",
      "processing chunk 3 out of 6\n",
      "processing chunk 4 out of 6\n",
      "processing chunk 5 out of 6\n"
     ]
    }
   ],
   "source": [
    "chapter_range_start = 28\n",
    "chapter_range_end = 32\n",
    "save_people_named_in_chapters(chapter_range_start=chapter_range_start, chapter_range_end=chapter_range_end, chapter_start_and_end = chapter_start_and_end, pdf = pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to ensure the LLM creates output in a consistent format (it doesn't matter how nicely you ask). There are two approaches to work with this, one would be to run a second ChatGPT call to check if the output was formatted correctly and if not to fix it, the other is just to do it manually. Given that the number of times GPT provided output in a different format, I opted just to fix the issues manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all.csv', 'Columns mismatch'), ('all_v1.csv', 'Columns mismatch')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_bad_csv_files('./outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the output files are formatted correctly, they are concatenated into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csvs(directory):\n",
    "    directory_path = Path(directory)\n",
    "    csv_files = directory_path.glob('*.csv')\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for filepath in csv_files:\n",
    "        df = pd.read_csv(filepath, encoding='utf-8', delimiter='|')\n",
    "        df.columns = df.columns.str.strip()  # remove leading and trailing whitespaces from column names\n",
    "        df = df[df[\"Name\"].str.strip() != \"-----\"] # remove rows which had markdown formatting        \n",
    "        df = df[df[\"Name\"].str.strip() != \"----\"] # remove rows which had markdown formatting        \n",
    "        df = df[df[\"Name\"].str.strip() != \"---\"] # remove rows which had markdown formatting\n",
    "        df = df[df[\"Name\"].str.strip() != \"-\"] # remove rows which had markdown formatting\n",
    "        df[\"Chapter\"] = filepath.stem\n",
    "        df_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combine_csvs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# output the number of rows in df\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_csvs\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./outputs/all.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combine_csvs' is not defined"
     ]
    }
   ],
   "source": [
    "# output the number of rows in df\n",
    "df = combine_csvs('./outputs')\n",
    "print(df.shape)\n",
    "df.to_csv(\"./outputs/all.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real manual work begins here. The consolidated list needs to be cleaned. Even though the system prompt asked only for information on people, the AI added organisations. Sometimes it would only list a person by their first or last name, sometimes both. Sometimes the list would include a title, other times it did not. I opted for a manual exercise to remove items I was not interested in and to ensure the names that were left were realitvely standard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "manual_list = pd.read_csv(\"./outputs/all_v1.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique names in the column manual_list[\"Name\"]\n",
    "manual_list[\"Name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 entries for ANC\n",
      "Aggregate Sentiment Score: 1\n",
      "\n",
      "Aggregate Reason: The overall sentiment towards the ANC is negative, with the text criticising the party for being stuck in the past, lacking business acumen leading to counterproductive legislation, corruption, and being politically sensitive towards dealing with issues. The sentiment is also negatively impacted due to the party's strict procurement and labour laws pushing investors away, and the general negative opinion about politicians' unwillingness to hear the truth.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "# get a list of all elements in manual_list that have \"name\" equal to \"Cyril Ramaphosa\"\n",
    "name = \"ANC\"\n",
    "filename = \"./aggregate/\" + name + \".txt\"\n",
    "subset = manual_list[manual_list[\"Name\"] == name]\n",
    "if len(subset) > 1:\n",
    "    print(\"Found \" + str(len(subset)) + \" entries for \" + name)\n",
    "    pipe_delimited_string = subset.to_csv(encoding='utf-8', index=False, sep='|')\n",
    "\n",
    "    system_instruction = \"The user will provide a table with the sentiment analysis for one person or organization from various places in a book. Scores in the Sentiment column are on a scale from 0 (extremely negative) to 10 (extremely positive). Please provide one overall sentiment analysis and one consolidated, summary Reason for the person or organization by aggregating all these into a single item. The one aggregate Sentiment score and reason should weight Sentiment Scores more highly if the Reason they have is compelling. The one aggregate Sentiment score should discount Neutral Ratings or ratings that correspond to Reasons that are not compelling. The aggregate Reason should be a short summary that supports the score. Do not provide any text other than the Aggregate Score and Aggregate Reason.\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instruction},\n",
    "            {\"role\": \"user\", \"content\": pipe_delimited_string},\n",
    "        ]\n",
    "    )\n",
    "    output = response['choices'][0]['message']['content']\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        f.write(output)\n",
    "\n",
    "elif len(subset) == 1:\n",
    "    print(\"Found one entry for \" + name)\n",
    "    # create the output string using subset[\"Sentiment\"] and subset[\"Reason\"]\n",
    "    output = \"Sentiment: \" + str(subset.loc[\"Sentiment\"][0]) + \"\\nReason: \" + subset.loc[\"Reason\"][0]\n",
    "    with open(filename, \"w\", encoding='utf-8') as f:\n",
    "        f.write(output)\n",
    "else:\n",
    "    print(\"No entry found for \" + name)\n",
    "    output = \"\"\n",
    "\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "913cc51b0c97a871649244c70ef3bb0367d000ab952a949e1ff5c01026bbb065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
