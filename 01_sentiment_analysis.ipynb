{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have placed some of the less interesting code in the `convert_book_to_text.py` file to try to keep the notebook flowing. Note that almost all of the code in that file was generated by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for the reload of all the functions in the file convert_pdf_to_text.py\n",
    "from importlib import reload\n",
    "import convert_book_to_text\n",
    "\n",
    "reload(convert_book_to_text)\n",
    "from convert_book_to_text import find_chapter_pages, \\\n",
    "                                 find_page_starting_with,\\\n",
    "                                 extract_text_from_pages_inclusive, \\\n",
    "                                 build_chapter_dictionary, \\\n",
    "                                 remove_chapter_header, \\\n",
    "                                 chunk_text, \\\n",
    "                                 find_bad_csv_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For copyright reasons I have not included the ebook in the repo. You should buy your own copy if you want to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n",
    "input_file = Path('./inputs/Truth to Power_ My Three Years Inside Eskom.pdf')\n",
    "input_file = Path(input_file)\n",
    "# Instantiate PdfReader object\n",
    "pdf = PyPDF2.PdfReader(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very specific to the format of the book I am processing. Not making this general is a design decision. This repo needs back and forward with a human\n",
    "# to work so spending too much time generalising functions seems like a waste of time.\n",
    "\n",
    "# Note: This is quite slow. It takes about 30 seconds to run on my machine.\n",
    "pages_with_numbers = find_chapter_pages(pdf)\n",
    "pages_with_numbers.append(find_page_starting_with(pdf, \"Index\"))\n",
    "# Create a dictionary where each chapter is saved with its start and end page\n",
    "chapter_start_and_end = build_chapter_dictionary(pages_with_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is for illustration. It allows the user to see what is happening in the main loop below, but for a single chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chapter:  2 from page 12 to page 18\n"
     ]
    }
   ],
   "source": [
    "chapter_number = 2\n",
    "start = chapter_start_and_end[chapter_number][\"start_page\"]\n",
    "end = chapter_start_and_end[chapter_number][\"end_page\"]\n",
    "print(\"processing chapter: \", str(chapter_number) + \" from page \" + str(start) + \" to page \" + str(end))\n",
    "text = remove_chapter_header(extract_text_from_pages_inclusive(pdf, start, end))\n",
    "chunked_text = chunk_text(text, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sentiment_analysis\n",
    "\n",
    "reload(sentiment_analysis)\n",
    "from sentiment_analysis import save_people_named_in_chapters, \\\n",
    "                               combine_csvs, \\\n",
    "                               aggregate_sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop. Chapters start at 1 and end at 31. chapter_range_end should be set to the number after the last chapter you want to process so, if you are processing the last chapter, chapter 31, only you would set `chapter_range_start = 31` and `chapter_range_end = 32`.\n",
    "\n",
    "The output of this is a list of CSV files in the folder `./output/`. A separate file is created for each chunk. \n",
    "\n",
    "The function sets a chunk size of 800 words. There is no science in determining how long this should be but we need to ensure there are sufficient tokens for the input, the system command and the output. I started with a chunk size of 1000 words but I found there were times where GPT3.5 would truncate the output. 800 seemed reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chapter:  1 from page 9 to page 11\n",
      "The file sentiment_detail\\chapter_1_0.csv already exists so skipping it\n",
      "The file sentiment_detail\\chapter_1_1.csv already exists so skipping it\n",
      "processing chapter:  2 from page 12 to page 18\n",
      "The file sentiment_detail\\chapter_2_0.csv already exists so skipping it\n",
      "The file sentiment_detail\\chapter_2_1.csv already exists so skipping it\n",
      "The file sentiment_detail\\chapter_2_2.csv already exists so skipping it\n",
      "The file sentiment_detail\\chapter_2_3.csv already exists so skipping it\n",
      "The file sentiment_detail\\chapter_2_4.csv already exists so skipping it\n"
     ]
    }
   ],
   "source": [
    "chapter_range_start = 1\n",
    "chapter_range_end = 3\n",
    "output_folder = \"./sentiment_detail\"\n",
    "save_people_named_in_chapters(chapter_range_start=chapter_range_start, \\\n",
    "                              chapter_range_end=chapter_range_end, \\\n",
    "                              chapter_start_and_end = chapter_start_and_end,\\\n",
    "                              pdf = pdf, \\\n",
    "                              output_folder = output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to ensure the LLM creates output in a consistent format (it doesn't matter how nicely you ask). There are two approaches to work with this, one would be to run a second ChatGPT call to check if the output was formatted correctly and if not to fix it, the other is just to do it manually. Given that the number of times GPT provided output in a different format, I opted just to fix the issues manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('all.csv', 'Columns mismatch'), ('all_v1.csv', 'Columns mismatch')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_bad_csv_files(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the output files are formatted correctly, they are concatenated into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1063, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# output the number of rows in df\n",
    "df = combine_csvs(output_folder)\n",
    "print(df.shape)\n",
    "df.to_csv(output_folder + \"/combined/all.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real manual work begins here. The consolidated list needs to be cleaned. Even though the system prompt asked only for information on people, the AI added organisations. Sometimes it would only list a person by their first or last name, sometimes both. Sometimes the list would include a title, other times it did not. I opted for a manual exercise to remove items I was not interested in and to ensure the names that were left were realitvely standard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "manual_list = pd.read_csv(output_folder + \"/combined/all_v1.csv\", encoding='utf-8')\n",
    "# count the number of unique names in the column manual_list[\"Name\"]\n",
    "manual_list[\"Name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 entries for Gwede Mantashe\n"
     ]
    }
   ],
   "source": [
    "name = \"Gwede Mantashe\"\n",
    "output_folder = \"./aggregate\"\n",
    "output = aggregate_sentiment(name = name, output_folder = output_folder, manual_list = manual_list)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "913cc51b0c97a871649244c70ef3bb0367d000ab952a949e1ff5c01026bbb065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
